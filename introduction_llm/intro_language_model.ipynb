{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978c7f0c0cdd9d35",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction (Causal) Language Models\n",
    "\n",
    "Since the success of ChatGPT at the latest, we almost all know how the technology roughly works. Many providers offer low-code or even no-code options that make the use of language models more accessible than ever. In this presentation, we want to implement some ideas ourselves to develop a better understanding of this technology. The aim is not to build the most powerful language model, but to concentrate on the ideas.\n",
    "\n",
    "The content of the talk is:\n",
    "\n",
    "1. Understanding the preprocessing step (Tokenization).\n",
    "2. Discuss what a causal language model actually models.\n",
    "3. How to generate text from a causal language model.\n",
    "4. How to fine-tune a model to learn a downstream task.\n",
    "5. How to mitigate hallucination in knowledge intensive tasks with Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "---\n",
    "\n",
    "**Content**\n",
    "\n",
    "1. [Preprocessing: Tokenization of text](#Preprocessing:-Tokenization-of-text-(Input-representation-for-Language-Models))\n",
    "2. [Causal Language Model](#Causal-Language-Model)\n",
    "3. [Generate text from a causal model and its strategies](#Generate-text-from-a-causal-model-and-its-strategies)\n",
    "4. [How to train a Language model](#How-to-train-a-Language-model)\n",
    "5. [Retrieval Augmented Generation (RAG)](#Retrieval-Augmented-Generation-(RAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119f8019db9d131",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk.tokenize\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "# import the transformer package from Huggingface\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from plot_utils import pareto_plot\n",
    "\n",
    "# use this `'cuda'` instead of `'mps'` if you have a cuda compatible GPU. If you want to run the code on the Mac > M1 use MPS.\n",
    "# if none of it holds true, use `'cpu'`\n",
    "device = torch.device('mps')\n",
    "\n",
    "# set to `True` if you want to train a model, this is need to execute certain cells.\n",
    "with_fine_tuning = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f925c73971ed2a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocessing: Tokenization of text (Input representation for Language Models)\n",
    "\n",
    "A Tokenizer is the interface between the raw text and the language model. A tokenizer decomposes text into smaller text chunks, called **tokens**, which then are encoded to **token ids** (non-negative integer) through a look-up table.\n",
    "\n",
    "<img src=\"../images/tokenizer_schema.jpg\">\n",
    "\n",
    "(Image [Andrej Karpathys - Let's build the GPT Tokenizer](https://youtu.be/zduSFxRajkE?si=V4SKTxELBiFGx5Kc))\n",
    "\n",
    "**Important** The text units/tokens the tokenizer knows is fixed (and typically finite). This fixed set of tokens is called the _vocabulary_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4d3a9a9e37644",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Character-Level Tokenization (Unicode Code Point Encoding)\n",
    "The most canonical way to split text/strings into smaller units is to decompose the string by its characters and use the Unicode Encoding as encoding.\n",
    "The [Unicode Standard](https://en.wikipedia.org/wiki/Unicode) defines 149813 characters (May 2024). Hence, the vocabulary size of the character-level tokenizer is 149813: each character defines one token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890de66f619a25eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenization on character level: character -> unicode\n",
    "def string_unicode_code_point_seq(text: str) -> list[int]:\n",
    "    return [ord(c) for c in text]\n",
    "\n",
    "\n",
    "def unicode_code_point_seq_string(unicode_code_point_seq: list[int]) -> str:\n",
    "    return \"\".join([chr(cp) for cp in unicode_code_point_seq])\n",
    "\n",
    "\n",
    "first_example = \"Some random sentence ðŸ˜€ðŸ˜‡\"\n",
    "encoded_example = string_unicode_code_point_seq(first_example)\n",
    "\n",
    "print(\"Text:\", first_example)\n",
    "print(\"Unicode Code Point sequence:\\n\", encoded_example)\n",
    "print(\"Length Unicode Code Point sequence:\", len(encoded_example))\n",
    "print(\"Decoded Unicode Code Point Sequence:\", unicode_code_point_seq_string(encoded_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562da2d991f3b54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Byte Level Tokenization (UTF-8 Encoding)\n",
    "We can reduce the size of the encoding even more by using UTF-8 Encoding. UTF-8 Encoding encodes each Unicode Code Point into at most 4 bytes. A byte-level tokenizer based on UTF-8 has only 256 tokens in the vocabulary: Each byte forms one token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d81baa50d2c99",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def string_byte_seq(text: str) -> list[int]:\n",
    "    return list(text.encode(\"utf-8\"))\n",
    "\n",
    "def byte_seq_string(byte_seq) -> str:\n",
    "    return bytes(byte_seq).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "uft_8_encoded_example = string_byte_seq(first_example)\n",
    "\n",
    "print(\"Text:\", first_example)\n",
    "print(\"Bytes sequence:\\n\", uft_8_encoded_example)\n",
    "print(\"Length bytes sequence:\\n\", len(uft_8_encoded_example))\n",
    "print(\"Decoded bytes sequence:\\n\", byte_seq_string(uft_8_encoded_example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6677285ad6c55",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What we observe is that UTF-8 encoding reduces the number of integers we need to encode our text. However, it increases the length of the encoding sequence. The reason is that newer Unicode Code Points need more than one byte to encode these Unicode Code Points:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901ed3a32a9204a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Subword-Level Tokenization\n",
    "\n",
    "Previous research indicates that language models generally perform better when modeled at the word level instead of character level (e.g. [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)).\n",
    "\n",
    "However, word-level tokenization has drawbacks: \n",
    "  - The Unicode standard includes nearly 150,000 elements, leading to potentially massive vocabularies.\n",
    "  - To manage size, elements like emojis are often excluded.\n",
    "  - Language evolution, especially with new internet slang and abbreviations, poses challenges as new terms are constantly introduced, which a word tokenizer might not recognize, resulting in unknown tokens (Out-of-Vocabulary (OOV) Words).\n",
    "\n",
    "Given the limitations of both word and character-level tokenization, modern language models often employ methods which interpolate between word and character(byte) levels to optimize both performance (of the language model) and vocabulary management. \n",
    "These methods are called **subword tokenizers**. Subword tokenizers decompose text into subwords instead of words which enables them to effectively handle the morphological variations by breaking words into subwords.\n",
    "\n",
    "Below you see a typical example of a subword tokenizer (Each coloured field marks a token).\n",
    "\n",
    "<img src=\"../images/tokenizer.jpg\">\n",
    "\n",
    "(Image GPT-2 Tokenizer)\n",
    "\n",
    "Prominent subword tokenizers are\n",
    "\n",
    "- BPE (BPE = Byte Pair Encoding)\n",
    "- WordPiece\n",
    "- SentencePiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a291075fe442d2c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## How Does A Subword Tokenizer Work?\n",
    "\n",
    "The tokenization process typically consists of at least the following steps: \n",
    "\n",
    " - **Pre-Tokenization** Decompose the raw text into \"words\".\n",
    " - **Model** Encode \"words\" into a byte or character sequence (as seen above). Then merge byte/character pairs successively into larger tokens of the vocabulary.\n",
    "\n",
    "<img src=\"../images/tokenizer_steps.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d171e21b2fcfdd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the tokenizer for a gpt2 model which is a byte-level BPE\n",
    "model_checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# get the vocabulary size of the tokenizer\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98594f303a756dd2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size of the {model_checkpoint}-Tokenizer: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2fd0c844630fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pre-Tokenization\n",
    "\n",
    "A first step is the **Pre-Tokenization**. In this step, the text is decomposed into smaller entities (typically word like chunks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8957d0cf2b4af0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_text = \"How do tokenizers work?\"\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3cc1dc76cdfecc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model\n",
    "\n",
    "In this step the pre-tokenized text chunks are encoded into unicode code points or bytes, as seen above. Then byte/character pairs are merged successively into larger tokens (based on a certain **merging rule**).\n",
    "\n",
    "**Important** The **merging rule** is a trained parameter of a subword tokenizer. It is trained by the compression algorithm [Byte Pair Encoding](#https://en.wikipedia.org/wiki/Byte_pair_encoding) (BPE): Starting from a small vocabulary, say the vocabulary of the UTF-8 encoder, the BPE merges the most frequent pairs of tokens in the text data into a new token. This process stops if a pre-defined vocabulary size is reached. \n",
    "\n",
    "Hence, the **merging rule** depends on the data, the BPE is trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4cc4a0eab8b5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tiktoken is the implementation of OpenAI's tokenizers\n",
    "# beside fast implementations of several tokenizers, the package also contains educational code\n",
    "from tiktoken._educational import bpe_encode  # noqa\n",
    "\n",
    "pre_tokenized_string = \"Ä tokenizers\"\n",
    "\n",
    "mergeable_ranks = {\n",
    "    tok.encode(\"utf-8\"): rank \n",
    "    for tok, rank in tokenizer.get_vocab().items()  # get the vocabulary of our tokenizer\n",
    "}\n",
    "bpe_encode(mergeable_ranks, pre_tokenized_string.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e8a46a114870d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251653f25b2bbfdd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_text = \"\"\"Today it rains.\n",
    "The rain.\n",
    "rain\n",
    "Rain\n",
    "raining\n",
    "\"\"\"\n",
    "\n",
    "dict(zip(tokenizer.encode(small_text), (tokenizer.decode(token_id) for token_id in tokenizer.encode(small_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5d5090df9288b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, token_idx in enumerate([0, 1, 2, 4, 200, 300, 5000, vocab_size// 2, vocab_size//2 + 1, vocab_size//2 + 2, vocab_size-2, vocab_size -1]):\n",
    "    print(f\"Example {i}:\", f\"Token Id {token_idx},\", f\"Token '{tokenizer.decode(token_idx)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbf5ed246d3399",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Beside the usual \"tokens\", there are several other technical (control) tokens that give the corresponding language model a certain signal:\n",
    "\n",
    "- EOS Token: End of String (Text) Token.\n",
    "- SEP Token: Separation Token.\n",
    "- UNK Token: Unknown Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabfe5ae4fcea5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c59bd2a8a40bae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is a nice visualisation for tokenizers. Let's try it out.\n",
    "\n",
    "```\n",
    "Let us make some experiments with given tokenizers.\n",
    "\n",
    "How does the same word behave under the given tokenizer.\n",
    "\n",
    "Today it rains.\n",
    "The rain.\n",
    "rain\n",
    "Rain\n",
    "raining\n",
    "\n",
    "Some calculations:\n",
    "\n",
    "11 + 23 = 34\n",
    "1045 + 145 = 1090\n",
    "10000000 + 10 = 10000010\n",
    "\n",
    "Some bad code:\n",
    "\n",
    "def bad_even_check(n: int) -> bool:\n",
    "   if n % 2 == 0:\n",
    "      return True\n",
    "   else:\n",
    "      return False\n",
    "\n",
    "The equivalent function\n",
    "def bad_even_check(n: int) -> bool:\n",
    "   if n%2==0:\n",
    "      return True\n",
    "   else:\n",
    "      return False\n",
    " ```\n",
    "in [Tokenizer Visualisation](https://tiktokenizer.vercel.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7986f136f394279",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Implication: Language-Specific Dependencies\n",
    "\n",
    "Subword tokenizers are probabilistic tokenizers that are trained on specific text data. This training introduces dependencies on the language of the text used.\n",
    "\n",
    "English, being the dominant language in most datasets, often results in longer token lengths for English words compared to other languages when using subword tokenization.\n",
    "\n",
    "The difference in token length has direct financial implications, since many services charge based on the number of tokens:\n",
    "\n",
    "- [OpenAI Pricing](#https://openai.com/api/pricing/)\n",
    "- [Mistral AI](#https://mistral.ai/technology/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c5978a325d133",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_tokenize(text: str) -> list[int]:\n",
    "    return nltk.tokenize.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecf98a6bd0da8f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_example_text = \"\"\"Byte pair encoding (also known as digram coding) is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling. Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words). This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial \"tokens\"). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.\"\"\"\n",
    "\n",
    "# with deepl translated german text\n",
    "german_translated_text = \"\"\"Die Byte-Paar-Kodierung (auch Digram-Kodierung genannt) ist ein Algorithmus, der erstmals 1994 von Philip Gage fÃ¼r die Kodierung von Textstrings in Tabellenform zur Verwendung bei der nachgelagerten Modellierung beschrieben wurde. Seine Modifikation ist bemerkenswert als der groÃŸe Sprachmodell-Tokenizer mit der FÃ¤higkeit, sowohl Token zu kombinieren, die einzelne Zeichen kodieren (einschlieÃŸlich einzelner Ziffern oder einzelner Satzzeichen) als auch solche, die ganze WÃ¶rter kodieren (sogar die lÃ¤ngsten zusammengesetzten WÃ¶rter). Bei dieser Modifikation wird im ersten Schritt davon ausgegangen, dass alle eindeutigen Zeichen eine anfÃ¤ngliche Menge von n-Grammen mit einer LÃ¤nge von einem Zeichen darstellen (d. h. anfÃ¤ngliche â€žTokenâ€œ). Dann wird nach und nach das hÃ¤ufigste Paar benachbarter Zeichen zu einem neuen, 2 Zeichen langen n-Gramm verschmolzen und alle Instanzen des Paares werden durch dieses neue Token ersetzt. Dies wird so lange wiederholt, bis ein Vokabular der vorgeschriebenen GrÃ¶ÃŸe erreicht ist. Es ist zu beachten, dass neue WÃ¶rter immer aus den Token des endgÃ¼ltigen Vokabulars und den Zeichen des Anfangssatzes gebildet werden kÃ¶nnen.\"\"\"\n",
    "\n",
    "\n",
    "print(\"Tokens per word (english):\", len(tokenizer.encode(english_example_text)) / len(word_tokenize(english_example_text)))\n",
    "print(\"Tokens per word (german):\", len(tokenizer.encode(german_translated_text)) / len(word_tokenize(german_translated_text)))\n",
    "\n",
    "print(f\"Mean Token length in characters (english): {np.mean([len(x) for x in tokenizer.tokenize(english_example_text)])}\")\n",
    "print(f\"Mean Token length in characters (german): {np.mean([len(x) for x in tokenizer.tokenize(german_translated_text)])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451585b2bbb5ef30",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Implication: Issues with simple tasks.\n",
    "\n",
    "Subword-level encodings have some drawbacks.\n",
    "\n",
    "- Since Tokenizers encode whole words, language models working on top of tokenizers have issue with spelling. Andrej Karpathy showed for example that the string `\".DefaultCellStyle\"` is one token for the GPT-4-Tokenizer. If you ask GPT-4 to ask how many \"i\" characters are contained in `\".DefaultCellStyle\"`, then it will give you a wrong answer. (The whole idea by Andrej Karpathy)\n",
    "- Language models working on top of subword tokenizers can not revert words.\n",
    "- Tokenizer often tokenize words with \"whitespace\" + \"word\" (e.g. `\" rain\"`). Since tokenizers are the interface of a language model adding whitespace add the end of text typically lower the performance of the text generation.\n",
    "\n",
    "\n",
    "## Remarks\n",
    "If you want to learn how to develop such a tokenizer from scratch, the following tutorial is recommended: [Andrej Karpathys - Let's build the GPT Tokenizer](https://youtu.be/zduSFxRajkE?si=V4SKTxELBiFGx5Kc)\n",
    "\n",
    "See also [TikToken](#https://github.com/openai/tiktoken/tree/main): TikToken implements all tokenizers that are used for GPT-3, GPT-3.5, GPT-4 and GPT-4o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca0e47fd536a8b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Causal Language Model\n",
    "\n",
    "A causal language model tries to estimate how a given text could be continued.\n",
    "\n",
    "* Simple example: The text `\"The capital of France is \"` is probably continued with the word \"Paris\".\n",
    "* Harder example: It is not clear at all how to continue the text `\"I like \"`. How the text could be continued certainly depends very much on who you ask. And even for one person it is hard to answer the question, because there are potentially many correct answers.\n",
    "\n",
    "But what we could do is to **rank** what words or tokens are rather likely to continue a given text and what words or tokens are not very likely to continue the text:\n",
    "\n",
    "* For example, `\"I like \"` could be continued with words like `apples, cars` etc. It is rather unlikely one would continue this text with `being ill, being thristy`. Furthermore, if you would continue the given text `\"I like \"` with words like `I` or `like`, it results in grammatically incorrect text.\n",
    "\n",
    "Hence, given a text and a vocabulary, we could potentially rank each word in the vocabulary to indicate what words are like to continue the text and what words are rather unlikely to continue the given text.\n",
    "\n",
    "## Formal Objective\n",
    "The formal objective of a causal language model is to determine a conditional probability measure:\n",
    "$$\n",
    "p(output \\vert input)\n",
    "$$\n",
    "where \"input\" refers to the initial segment of text and \"output\" is the subsequent token. This probability quantifies how likely a particular continuation is, given the preceding text.\n",
    "\n",
    "<img src=\"../images/language_model.jpg\" alt=\"Causal Language Model\" />\n",
    "\n",
    "In the context of language models, the \"input\" is typically a sequence of tokens, and the model's task is to estimate the next tokens probability distributions in the sequence based on its understanding of language structure and content. This is mathematically represented as:\n",
    "\n",
    "$$\n",
    "p(x_t \\vert x_{1}, \\dots, x_{t-1})\n",
    "$$\n",
    "\n",
    "where $x_t$ is the token at position $t$, and the sequence $[x_{1}, \\dots, x_{t-1}]$ constitutes the input or context.\n",
    "\n",
    "A causal language model is therefore technically simply a mathematical function of the following type:\n",
    "$$\n",
    "p\\colon \n",
    "\\begin{cases}\n",
    "    \\left\\{ [x_1,\\dots, x_t] \\vert t \\geq 1,\\, x_i \\in \\mathcal{V} \\right\\} \\to \\lbrace (p_w)_{w\\in \\mathcal{V}} \\mid p_w \\geq 0, \\, \\sum_{w\\in \\mathcal{V}} p_w = 1 \\rbrace \\\\\n",
    "    [x_1, \\dots, x_t] \\mapsto (p(x\\vert x_1, \\dots, x_t))_{x\\in \\mathcal{V}}\n",
    "  \\end{cases}\n",
    "$$\n",
    "where $\\mathcal{V}$ is the vocabulary (set of tokens).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab945275a16da524",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Why are neural networks used to represent a language model?\n",
    "\n",
    "Language models rely on tokenizers. As seen above, tokenizers break text into tokens without capturing the underlying semantics of the words. For instance, different forms or inflections of the same word stem are assigned distinct tokens, each with a unique encoding.\n",
    "\n",
    "**Example 1** Variants of the word \"rain\" receive different tokens and encodings:\n",
    "\n",
    "| Tokenizer | Word stem | Token     | Encoding |\n",
    "|-----------|----------|-----------|--------|\n",
    "| GPT-2     | \"rain\"    | ' rains'  | 29424  |\n",
    "| GPT-2     | \"rain\"    | ' rain'   | 6290   |\n",
    "| GPT-2     | \"rain\"    | 'rain'    | 3201   |\n",
    "| GPT-2     | \"rain\"    | 'Rain'    | 31443  |\n",
    "| GPT-2     | \"rain\"    | 'raining' | 24674  |\n",
    "\n",
    "**Example 2** Different forms of the verb \"go\" are encoded differently:\n",
    "\n",
    "| Tokenizer | Lemma  | Token    | Encoding |\n",
    "|-----------|--------|----------|----------|\n",
    "| GPT-2     | \"go\"   | 'go'     | 2188     |\n",
    "| GPT-2     | \"go\"   | 'went'   | 19963    |\n",
    "| GPT-2     | \"go\"   | 'going'  | 5146     |\n",
    "\n",
    "**Example 3** Related words are assigned distinct tokens and encodings:\n",
    "\n",
    "| Tokenizer | Related Words | Tokens       | Encoding     | Category     |\n",
    "|-----------|---------------|--------------|--------------|--------------|\n",
    "| GPT-2     | \"cat\", \"dog\"  | 'cat', 'dog' | 9246, 9703   | Animal, Pet  |\n",
    "| GPT-2     | \"run\", \"walk\" | 'run', 'walk' | 5143, 11152  | motion verb  |\n",
    "\n",
    "\n",
    "Given these examples, it is apparent that token encodings by tokenizers lack efficiency in capturing language nuances. To address this, tokens can be represented as points in a multidimensional vector space:\n",
    "\n",
    "$$\n",
    "\\mathbb{R}^d = \\lbrace (r_1, \\dots, r_d) \\vert r_i \\in \\mathbb{R} \\rbrace,\n",
    "$$\n",
    "\n",
    "where $d$ is a positive integer. \n",
    "\n",
    "This vector space framework enables a concept of **closeness** between tokens. Metrics such as **Euclidean distance** or **cosine similarity** quantify how close or distant the tokens are, allowing semantically similar verbs like \"go\" and \"goes\" to be positioned near each other, whereas unrelated words are placed far apart. (see [Word Embedding Visualization](#https://projector.tensorflow.org/))\n",
    "\n",
    "Converting the discrete token encodings into a continuous vector representation supports modeling the conditional expectation as a continuous function, preserving semantic closeness so that small changes in the input result in small, meaningful shifts in the output.\n",
    "\n",
    "**For example**: The sentences `\"I walk into a room\"` and `\"I go into a room\"` could probably be continuate quite similar.\n",
    "\n",
    "<img src=\"../images/neural_network_lm.jpg\" alt=\"Causal Language Model\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb113cf9feea1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load pre-trained Causal Language Model\n",
    "\n",
    "Neural network based language models are basically distinguishable by two main aspects:\n",
    "\n",
    "* The architecture of the neural network which determines how the different _layers_ or _blocks_ are composed and how large these _layers_ or _blocks_ are. A typical building _block_ of a neural network consists of a matrix (a two-dimensional numeric array), a bias vector (one-dimensional array) and a so-called activation function. The values of the matrix and the bias vector are called _weights_ and are the parameters that are actually updated during training a model.\n",
    "* The concrete _weights_ of model which are updated during the training and store the learned information. \n",
    "\n",
    "The architecture of a model determines (among other things) how large a model is in terms of the number of weights, and therefore also affects memory usage.\n",
    "\n",
    "The saying â€˜more is moreâ€™ is quite true for language models based on neural networks. Large models with many weights typically perform better than smaller models when modeling language. This is because they can capture more complex patterns and nuances in the data. However, the trade-off is that high-performance language models with many parameters rarely fit on ordinary GPUs due to their substantial memory requirements. \n",
    "\n",
    "| Model       | Number of Weights | Memory Size (weight precision float32) |\n",
    "|-------------|-------------------|----------------------------------------|\n",
    "| GPT2-small  | 0.124 B           | 0.5 GB                                 |\n",
    "| GPT2-medium | 0.355 B           | 1.42 GB                                |\n",
    "| PHI-3-mini  | 3.8 B             | 14.2 GB                                |\n",
    "| Llama3-8B   | 8.03 B            | 32 GB                                  |\n",
    "| Llama3-70B  | 70.6 B            | 280 GB                                 |\n",
    "| GPT 3       | 174 B             | 696 GB                                 |\n",
    "| GPT 4       | 1760 B            | 7 TB                                   |\n",
    "\n",
    "---\n",
    "\n",
    "We choose a quite small language model as you can see below (so that it fit onto each device).\n",
    "\n",
    "To load the language model we again use Huggingface's `transformer` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0baacbc5298b7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# load pre-trained model from a HuggingFace repository\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, device_map=device)\n",
    "\n",
    "# set the model to evaluation mode to deactivate technical \"modules\" (like dropout)\n",
    "model.eval() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695b7e2113cbf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Number of model weights of {model_checkpoint}: {model.num_parameters() / 1e6:.3f} million\")\n",
    "print(f\"Model foodprint of {model_checkpoint}: {model.get_memory_footprint()/ 1e9:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a38f7210af19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following link you can use to estimate VRAM usage. [Memory Estimator](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf68b11c2bbf47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Embedding\n",
    "\n",
    "Let us have a look how our loaded model embeds tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e23df013cf8e4",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token = \"go\"\n",
    "\n",
    "EMBEDDING_LAYER_NAME = {\n",
    "    \"gpt2\": \"transformer.wte\",\n",
    "    \"keeeeenw/MicroLlama\": \"model.embed_tokens\",\n",
    "}\n",
    "\n",
    "def get_module_by_name(name: str, model) -> torch.nn.Module | None:\n",
    "    for module_name, module in model.named_modules():\n",
    "        \n",
    "        if module_name != name:\n",
    "            continue\n",
    "        \n",
    "        return module\n",
    "\n",
    "with torch.no_grad():\n",
    "    token_embedding = get_module_by_name(EMBEDDING_LAYER_NAME[model_checkpoint], model)(tokenizer.encode(token, return_tensors=\"pt\").to(model.device))\n",
    "\n",
    "print(f\"Shape of the embedding tensor: {tuple(token_embedding.shape)}\")\n",
    "print(token_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd5953096b8022",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token = \" dog\" # \"go\" #  \"people\" # \"night\" # \n",
    "\n",
    "@torch.no_grad()\n",
    "def get_closest_by_euclidean_distance(word: str, embedding_module_name: str, model, tokenizer, topk: int = 10) -> dict[str, list]:\n",
    "    \n",
    "    # get the embedding module from the neural network language model\n",
    "    embedding = get_module_by_name(embedding_module_name, model)\n",
    "    assert embedding is not None, \"No embedding found\"\n",
    "    \n",
    "    # get the token id for `word`\n",
    "    word_token = tokenizer.encode(word, return_tensors=\"pt\")\n",
    "    word_token = torch.tensor(\n",
    "        [\n",
    "            token_id.item() for token_id in word_token.view(-1) if token_id not in tokenizer.all_special_ids\n",
    "        ]\n",
    "    ).to(model.device)\n",
    "\n",
    "    assert word_token.shape[-1] == 1, \"`word` decomposes into more than one token.\"\n",
    "\n",
    "    # embed the `word`\n",
    "    word_embedding = embedding(word_token)\n",
    "    \n",
    "    # embed the complete vocabulary (all token ids)\n",
    "    all_embedded_tokens = embedding(torch.tensor(list(range(tokenizer.vocab_size))).to(model.device))\n",
    "    \n",
    "    # get the Euclidean distance between the embedded `word` and all other embeddings\n",
    "    dists = torch.norm(word_embedding - all_embedded_tokens, dim=-1).view(-1)\n",
    "    \n",
    "    # get the top closest tokens \n",
    "    values, token_ids = torch.topk(-dists, k=topk + 1)\n",
    "    \n",
    "    return {\n",
    "        \"Token\": [tokenizer.decode(token_id) for token_id in token_ids[1:]],\n",
    "        \"Distance\": [-val.item() for val in values[1:]]\n",
    "    }\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    y='Distance', \n",
    "    hue='Token', \n",
    "    data=pd.DataFrame(get_closest_by_euclidean_distance(token, EMBEDDING_LAYER_NAME[model_checkpoint], model, tokenizer, topk=20)), \n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "plt.title(f\"Tokens close to '{token}'\") \n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792137e5b2947a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Probability model output\n",
    "\n",
    "The output of a neural network-based language model is typically not a probability vector of the vocabulary size, but rather it consists of logits. Logits are logistic units or unnormalized log-probabilities and can be any real number. To interpret a sequence of logits correctly:\n",
    "\n",
    "$$\n",
    "\\text{logits} = (l_1, \\dots, l_V)\n",
    "$$ \n",
    "\n",
    "we must apply the _softmax function_, which is a normalized exponential function:\n",
    "\n",
    "$$\n",
    "\\sigma(l_1,\\dots, l_V) = \\frac{1}{\\sum_{i=1}^V \\exp(l_i)} [ \\exp(l_1), \\dots,  \\exp(l_V) ],\n",
    "$$\n",
    "\n",
    "Here, $V$ is the vocabulary size.\n",
    "\n",
    "Another distinctive aspect of the output is influenced by the architecture of the **transformer** model. The shape of the output depends on the length of the input token sequence $[ x_1, \\dots, x_t ] $:\n",
    "\n",
    "$$\n",
    "[ x_1, \\dots, x_t ] \\xrightarrow[]{\\text{transformer}} \\begin{bmatrix} \\text{logits}_1 \\\\ \\vdots \\\\ \\text{logits}_t \\end{bmatrix} \n",
    "\\xrightarrow[]{\\sigma} \\begin{bmatrix} p(. \\vert x_1) \\\\ \\vdots \\\\ p(. |x_1,\\dots, x_t) \\end{bmatrix},\n",
    "$$\n",
    "In this model, the softmax function $\\sigma$ is applied to each row of the output logits matrix. This processing converts the logits into probabilities that predict the likelihood of each subsequent token given the prior sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427562aaa0ca299",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_text = \"Some test continue to write some\"\n",
    "print(\"Input tokens sequence:\", tokenizer.encode(example_text, return_tensors=\"pt\").to(device))\n",
    "model_output = model(tokenizer.encode(example_text, return_tensors=\"pt\").to(device)).logits\n",
    "print(\"Output shape:\", list(model_output.shape))\n",
    "print(\"Output:\", model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddf54458e7a7d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_top_k_from_transformer(\n",
    "    model: torch.nn.Module, tokenizer, input_text: str, k: int = 10, temperature: float = 1.0\n",
    ") -> dict[str, float]:\n",
    "    \n",
    "    # set model to evaluate model\n",
    "    model.eval()\n",
    "    \n",
    "    # return pytorch tensors\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # evaluate model\n",
    "    with torch.no_grad(): \n",
    "        outputs = model(inputs)\n",
    "    \n",
    "    # multiply temperature\n",
    "    logits = (1/temperature) * outputs.logits\n",
    "    \n",
    "    # get probabilities from the next token\n",
    "    probabilities = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "    \n",
    "    # get the k most likely \n",
    "    probs, token_ids = torch.topk(probabilities, k)\n",
    "    \n",
    "    return {token: val.item() for token, val in zip([tokenizer.decode(tok) for tok in token_ids.tolist()], probs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8d71681cd61d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot_utils import display_process\n",
    "\n",
    "input_text = \"Today is a sunny day and\"\n",
    "\n",
    "display(display_process([\n",
    "    input_text,\n",
    "    tokenizer.tokenize(input_text),\n",
    "    tokenizer.encode(input_text),\n",
    "    get_top_k_from_transformer(model, tokenizer, input_text, k=4)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486b7ca44393d7d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_text = \"I like to think\" # \"I like to think that I'm a good person.\" # \"Today is a sunny day and\"\n",
    "\n",
    "topk = get_top_k_from_transformer(model, tokenizer, input_text, k=20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set figure size\n",
    "sns.barplot(y='Probability', hue='Token', data=pd.DataFrame(\n",
    "    {\"Probability\": list(topk.values()), \"Token\": list(topk.keys())}\n",
    "), palette='viridis')  # Create bar plot\n",
    "\n",
    "plt.title('Probability of Next Tokens')  # Add title\n",
    "plt.xlabel('Probability')  # Label x-axis\n",
    "plt.ylabel('Next Token')  # Label y-axis\n",
    "plt.show()  # Show plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37399d6b7270f7c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate text from a causal model and its strategies\n",
    "\n",
    "How to generate text from a given (causal) language model?\n",
    "\n",
    "```\n",
    "Given an array of n tokens [x_1,..., x_n]\n",
    "Set context = [x_1,..., x_n]\n",
    "while True:\n",
    "  sample = draw sample from p(. | context)\n",
    "  context.append(sample)\n",
    "```\n",
    "\n",
    "## Context Length\n",
    "\n",
    "To generate text, we feed the language model iteratively with its own output to create longer text. This process involves using the model's previous output as input for generating the next token in the sequence.\n",
    "\n",
    "Keep in mind that the maximal context length of language models is typically limited. This means that the length of the token array you can feed into a language model has a bounded size.\n",
    "\n",
    "As with other specifications of a language model, the context length depends on the model. Different models have different maximum context lengths, which are determined by their architecture and design.\n",
    "\n",
    "| Model       | Maximal Context length |\n",
    "|-------------|------------------------|\n",
    "| GPT2-small  | 1024 Tokens            |\n",
    "| GPT2-medium | 1024 Tokens            |\n",
    "| PHI-3-mini  | 4000 Tokens            |\n",
    "| Llama3-8B   | 8000 Tokens            |\n",
    "| Llama3-70B  | 8000 Tokens            | \n",
    "| GPT 3       | 2048 Tokens            |\n",
    "| GPT 4       | 32000 Tokens           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35808c2f8f01e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Maximal context length of {model_checkpoint}: {tokenizer.model_max_length} Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4154ff407b0b8e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Top p\n",
    "\n",
    "Language models based on neural networks, when given a context $[x_1, \\dots, x_t]$ and a threshold $\\varepsilon > 0$ (close to zero), typically reveal that a large number of tokens from the vocabulary have a probability\n",
    "\n",
    "$$ p(x\\vert x_1, \\dots, x_t) > \\varepsilon.$$ \n",
    "\n",
    "This characteristic suggests a strong generalization capacity of the model. However, relying solely on this broad probability distribution for text generation can lead to unpredictable outcomes. Specifically, tokens that are less likely to be the next logical choice might be randomly selected, which can result in nonsensical or incoherent text. This is particularly problematic when the goal is to generate fact-based content, where a more deterministic approach is preferable.\n",
    "\n",
    "To illustrate this point, let's consider an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c08a2dbe63362",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_text = \"The capital of france is\"\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# get token-likelihood mapping for the most k likely next tokens\n",
    "topk = get_top_k_from_transformer(model, tokenizer, input_text, k=tokenizer.vocab_size)\n",
    "\n",
    "# create pareto plot\n",
    "pareto_plot(\n",
    "    pd.DataFrame({'Token': list(topk.keys()), 'Percentage': list(topk.values())}), \n",
    "    'Token', \n",
    "    'Percentage', \n",
    "    topk=30\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ff35689df876",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The \"Top p\" Sampling Strategy\n",
    "\n",
    "As we have seen above, a large mass of the probability distribution is distributed over a large set of tokens that are rather unlikely to be a reasonable next token. Sampling directly from such a wide distribution can result in nonsensical or irrelevant text outputs, undermining the model's utility for practical applications.\n",
    "\n",
    "To mitigate this issue, the \"top p\" sampling method, also known as \"nucleus sampling,\" is employed. This method involves:\n",
    "\n",
    "- **Ordering Tokens by Likelihood**: Tokens are ranked according to their probability given the context $p(x\\vert x_1, \\dots, x_n)$.\n",
    "- **Cutoff at Cumulative Probability p**: \"Top p\" sampling focuses only on the subset of tokens that collectively make up the top $p%$ of the probability mass. This approach effectively narrows down the choice of tokens to those most likely to be contextually relevant, reducing the inclusion of implausible tokens.\n",
    "- **Deterministic Sampling within a Probabilistic Framework**: By limiting the scope to a more probable subset of tokens, the \"top p\" method allows for a more controlled and deterministic generation process while maintaining the inherent probabilistic nature of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463822a016d5fcd7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    # sort probability value\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    # \n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4ea4742358d6c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Temperature in Language Model Generation\n",
    "\n",
    "As previously discussed, a language model predicts the likelihood of the next token based on the provided context. When sampling from this probability distribution:\n",
    "\n",
    "1. If the model is very confident, the generation will tend to be deterministic.\n",
    "2. If the model is less confident, the generation will be more random or unpredictable.\n",
    "\n",
    "\n",
    "**Temperature** is a key parameter in controlling the predictability of text generation. High temperatures result in less predictable text generation, while low temperatures lead to more deterministic outputs.\n",
    "\n",
    "The application of temperature in text generation is described as follows:  Given a temperature $\\theta > 0$ and a vector of logits $(l_1, \\dots, l_V)$\n",
    "$$\n",
    "\\sigma\\left(\\frac{l_1}{\\theta}, \\dots \\frac{l_V}{\\theta} \\right) = \\frac{1}{\\sum_{i=1}^V \\exp({l_i/\\theta})} \\left[\\exp({l_1/\\theta}), \\dots, \\exp({l_V/\\theta})\n",
    "\\right],\n",
    "$$\n",
    "where $V$ is the vocabulary size and $\\sigma$ denotes the softmax function. This formula adjusts the logits by the temperature, effectively scaling the sharpness of the probability distribution.\n",
    "\n",
    "Let's visualize the impact of temperature for the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29ccf899fa2c0d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_text = \"The capital of france is\"\n",
    "topk = get_top_k_from_transformer(model, tokenizer, input_text, k=20, temperature=100.3)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set figure size\n",
    "sns.barplot(y='Probability', hue='Word', data=pd.DataFrame(\n",
    "    {\"Probability\": list(topk.values()), \"Word\": list(topk.keys())}\n",
    "), palette='viridis')  # Create bar plot\n",
    "\n",
    "plt.title('Probability of Next Words')  # Add title\n",
    "plt.xlabel('Probability')  # Label x-axis\n",
    "plt.ylabel('Next Word')  # Label y-axis\n",
    "plt.show()  # Show plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c1314e1f7b54c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Frequency penalty\n",
    "\n",
    "When generating text as described above from a causal language model. It can happen that the model start to repeating itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e69920392f642",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def freq_penalty(logits: torch.Tensor, contexts: torch.Tensor, alpha_penalty: float) -> torch.Tensor:\n",
    "    occurrences_vectors = []\n",
    "    \n",
    "    vocab_size = logits.shape[-1]\n",
    "    \n",
    "    for context in contexts:\n",
    "        occurrences_vector = torch.zeros(vocab_size, dtype=torch.long, device=logits.device)\n",
    "        \n",
    "        unique, counts_unique = torch.unique(context, return_counts=True)\n",
    "\n",
    "        occurrences_vector[unique[unique >= 0]] = counts_unique[unique >= 0]\n",
    "        \n",
    "        occurrences_vectors.append(occurrences_vector)\n",
    "        \n",
    "    occurrences = torch.stack(occurrences_vectors).to(logits.device)\n",
    "    \n",
    "    return logits - occurrences * alpha_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadaaef18a356338",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sum up to generate text\n",
    "\n",
    "We are now ready to implement a _generation_ function with which we generate text based on a language model.\n",
    "\n",
    "**Notes**: Note that a language model is \"stateless\" and does not have any capacity to memorize. To generate text, we always feed the language model with the complete context. In particular, the maximal context length of a language model limits the possibility to generate infinite text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2e9a900849b3f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_token(\n",
    "    model: torch.nn.Module, input_ids: torch.Tensor, top_p: float = 0.9, temperature=1.0, alpha_penalty=0.2\n",
    ") -> torch.Tensor:\n",
    "    # set model to evaluate model\n",
    "    model.eval()\n",
    "    \n",
    "    # evaluate model\n",
    "    with torch.no_grad(): \n",
    "        outputs = model(input_ids.to(model.device))\n",
    "    \n",
    "    # get logits from the model\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    logits = freq_penalty(logits[0, -1, :], input_ids, alpha_penalty)\n",
    "    \n",
    "    # calculate the probabilities from the logits\n",
    "    probability = torch.softmax((1/temperature) * logits, dim=-1)\n",
    "\n",
    "    # get the next token by sampling\n",
    "    next_token = sample_top_p(probability, top_p)\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model: torch.nn.Module, \n",
    "    tokenizer, \n",
    "    text: str, \n",
    "    max_gen_length: int = 124, \n",
    "    top_p: float = .9, \n",
    "    temperature: float = .6,\n",
    "    alpha_penalty: float = 0\n",
    ") -> tuple[torch.Tensor, list[int]]:\n",
    "    # encode initial context\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)  # shape [1, token_sequence_length]\n",
    "    \n",
    "    # instantiate sequence of output ids\n",
    "    out_ids = []\n",
    "    \n",
    "    for _ in range(max_gen_length):\n",
    "        \n",
    "        # get next token id by sampling\n",
    "        next_token = get_next_token(model, tokens, top_p, temperature, alpha_penalty)\n",
    "\n",
    "        # append next token id to `out_ids`\n",
    "        out_ids.append(next_token.item())\n",
    "        \n",
    "        # check escape conditions\n",
    "        max_gen_length_reached = len(out_ids) == max_gen_length\n",
    "        eos_token_reached = next_token.item() == tokenizer.eos_token_id\n",
    "        \n",
    "        if max_gen_length_reached or eos_token_reached:\n",
    "            break\n",
    "        \n",
    "        # add next token to context\n",
    "        tokens = torch.concat([tokens, next_token.view(1, -1)], dim=-1)\n",
    "    \n",
    "    return tokens, out_ids\n",
    "\n",
    "example_text = \"I like\" # \"The capital of france is\"\n",
    "tokens, out_ids = generate(model, tokenizer, example_text, max_gen_length=26, top_p=.3, temperature=100.8, alpha_penalty=10.)\n",
    "tokenizer.decode(out_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad794c27dd1ceff1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# How to train a Language model\n",
    "\n",
    "## 1. Pre-training\n",
    "**Objective**: Develop a broad understanding of language patterns and structures through extensive exposure to diverse textual data.\n",
    "\n",
    "**Process**: The model is trained on a large corpus, often with billions of words spanning various topics. \n",
    "\n",
    "## 2. Supervised Fine-Tuning\n",
    "After the pre-training step, the language model has a general understanding of language. The language model is then a **text completer**, i.e. it tries to continue input text. Unlike language models for chatbots, for example, which react to our input like a human conversation partner, the model simply completes text after the pre-training step.\n",
    "\n",
    "**Objective**: Adapt a pre-trained model to excel in specific tasks such as \n",
    "   * question-answering\n",
    "   * **following instructions**\n",
    "   * or summarizing text.\n",
    "\n",
    "**Process**: In fine-tuning, the general model is trained further on a smaller, task-specific dataset. The model is trained to learn specific prompt template and how it has to react on certain signals. This step is relatively faster and less resource-intensive than pre-training because the model is already knowledgeable about language fundamentals. During fine-tuning, the model parameters are adjusted to optimize performance for the specific task metrics.\n",
    "\n",
    "**Technical notes**: Training (pre-training/fine-tuning (there is no difference between these steps in the implementation of the training loop except certain hyperparameters like batch size, learning rate etc.)) a neural network is in almost all cases based on a gradient descent algorithm. \n",
    "\n",
    "## 3. Alignment Training\n",
    "Language models reproduce text on which they have been trained. This can lead to the generation of conspiracy theories, violent language or other false information and common misconceptions.\n",
    "\n",
    "**Objective**: Aligning model with human preferences to increase its utility and safety. By leveraging human or AI preference in the training loop, one can attain large improvements.\n",
    "\n",
    "**Process**: Uses different preference learning algorithms like \n",
    "- [Deep Reinforcement Learning from Human Preferences](#https://arxiv.org/pdf/1706.03741)\n",
    "- [Direct Preference Optimization](#https://arxiv.org/pdf/2305.18290)\n",
    "\n",
    "## Practical Notes \n",
    "\n",
    "**Domain specific models**: As described above, a concrete language model is determined by its architecture and its weights. The weights control the strengths and weakness of a model.\n",
    "\n",
    "| Model                               | Model Architecture | Task        | Good at                       | Link                                                       | Fine Tuned on                                               |\n",
    "|-------------------------------------|--------------------|-------------|-------------------------------|------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| microsoft/Phi-3-mini-128k-instruct  | PHI-3              | Instruction | Math, Code, Logical Reasoning | https://huggingface.co/microsoft/Phi-3-mini-128k-instruct  |                                                             |\n",
    "| nvidia/Llama3-ChatQA-1.5-70B        | Llama3             | QA/RAG      |                               | https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B         | https://huggingface.co/datasets/nvidia/ChatQA-Training-Data |\n",
    "| aaditya/Llama3-OpenBioLLM-70B       | Llama3             | Instruction | Bio medic                     | https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B       | biomedical data                                             |\n",
    "| shenzhi-wang/Llama3-8B-Chinese-Chat | Llama3             | Instruction | Responses in Chinese          | https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat |                                                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e06c7c416a0f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fine-Tuning our language model to learn a specific task\n",
    "\n",
    "Until now, the model we've been discussing primarily functions as a **text completer**. \n",
    "\n",
    "**Goal**: Train the model to respond to instructions.\n",
    "\n",
    "To achieve this, the model must learn to interpret **control tokens** and **prompt templates**. These elements act as signals, guiding the model to respond appropriately to given inputs. \n",
    "\n",
    "Here is an example of how that could look like for a Mistral Model.\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", use_fast=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"{user content}\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"{assistant response}\"},\n",
    "    {\"role\": \"user\", \"content\": \"{user content}\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.decode(tokenizer.apply_chat_template(messages)))\n",
    "```\n",
    "_Output_\n",
    "```\n",
    "'<s>[INST]  {user content} [/INST] {assistant response}</s>[INST]  {user content} [/INST]'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd7305bf7e562b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is no standardised approach in this context. Prompt templates depend on the model. See for example [microsoft/Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) for another prompt template example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b393ce7ef3948",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_dataset = load_dataset(\n",
    "    \"yahma/alpaca-cleaned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbba63e5458ee6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_dataset[\"train\"].to_pandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad77e7602da5083",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_CONTEXT_LENGTH = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc44967eca21ed8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create a specific prompt template\n",
    "\n",
    "As described above, we want to teach our model how to react to instructions. To make this possible, we need to establish a certain prompt template that gives the model the right signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53baaa1192d887",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prompt_template(example):\n",
    "    text = \"\"\n",
    "    if input_ := example.get(\"input\"):\n",
    "        text += f\"<Input> {input_}\"\n",
    "    if instruction := example.get(\"instruction\"):\n",
    "        text += f\"<Inst> {instruction}<Response>\"\n",
    "    if output := example.get(\"output\"):\n",
    "        text += f\"{output} </Response>\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d412bc10673e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template(\n",
    "    {\n",
    "        \"input\": \"The capital of germany is Berlin.\",\n",
    "        \"instruction\": \"What is the capital of germany?\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51007b12d64194",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template(\n",
    "    {\n",
    "        \"input\": \"The capital of germany is Berlin.\",\n",
    "        \"instruction\": \"What is the capital of germany?\",\n",
    "        \"output\": \"The capital of germany is Berlin.\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e3e54b2c7a2b1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faeef96e0a601cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prompt(example, tokenizer=None):\n",
    "    \n",
    "    text = prompt_template(example)\n",
    "    \n",
    "    if tokenizer:\n",
    "        text += tokenizer.eos_token\n",
    "    \n",
    "    example[\"text\"] = text\n",
    "    \n",
    "    return example\n",
    "\n",
    "\n",
    "text_dataset = text_dataset.map(prompt).filter(lambda ex: len(tokenizer.encode(ex[\"text\"])) <= MAX_CONTEXT_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664429e980e98c58",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7dabbd2d844dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_val_dataset = text_dataset[\"train\"].train_test_split(train_size=.9, seed=1901)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2166bdd2ce1d3d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf21a0d0c9aad34",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = None\n",
    "\n",
    "if with_fine_tuning:\n",
    "    from transformers import TrainingArguments\n",
    "    from trl import SFTTrainer\n",
    "    from peft import LoraConfig\n",
    "    \n",
    "    TARGET_MODULS = {\n",
    "        \"gpt2\": [\"wte\", \"c_attn\", \"c_proj\", \"c_fc\", \"c_proj\", \"lm_head\"],\n",
    "        \"phi3\": ['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "        \"keeeeenw/MicroLlama\": ['gate_proj', 'up_proj', 'down_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    }\n",
    "    \n",
    "    # lora config\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=TARGET_MODULS[model_checkpoint]\n",
    "    )\n",
    "    \n",
    "    # Set training arguments\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=os.path.join(\"..\", \"logs\", \"model_checkpoints\"),\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        logging_steps=200,\n",
    "        optim=\"adamw_torch\",\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=10,\n",
    "        # fp16=True  # does not work on mps devices\n",
    "    )\n",
    "    \n",
    "    # Set supervised fine-tuning parameters\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_val_dataset[\"train\"],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=MAX_CONTEXT_LENGTH,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84857c2d36ea6c1b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if trainer:\n",
    "    # train model\n",
    "    trainer.train()\n",
    "    \n",
    "    # save model\n",
    "    trainer.save_model(\"gpt2-instruction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efaad4fb19577c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_peft_model(model_checkpoint: str, adapters_name: str, device=None):\n",
    "    from peft import PeftModel\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_checkpoint,\n",
    "    )\n",
    "    \n",
    "    # load our fine-tuned model\n",
    "    peft_model = PeftModel.from_pretrained(model, adapters_name)\n",
    "    \n",
    "    # put model on specified device\n",
    "    if device is not None:\n",
    "        peft_model.to(device)\n",
    "    \n",
    "    # get the associated tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "    \n",
    "    return peft_model, tokenizer\n",
    "\n",
    "\n",
    "peft_model, tokenizer = load_peft_model(model_checkpoint, \"gpt2-instruction\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f71aa27b0e2a90",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's try our instruction model\n",
    "\n",
    "<img src=\"../images/meme.jpeg\" alt=\"Fine Tuned Model\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17901ee94de4785e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, out_ids = generate(\n",
    "    peft_model, \n",
    "    tokenizer, \n",
    "    prompt_template({\"input\": \"Answer the following instruction in 10 words.\", \"instruction\": \"How are you?\"}), \n",
    "    max_gen_length=50, top_p=.5, temperature=.9)\n",
    "print(tokenizer.decode(out_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac581a3bbb1168d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    _, out_ids = generate(\n",
    "        peft_model, \n",
    "        tokenizer, \n",
    "        prompt_template({\"instruction\": \"What is the capital of germany?\"}), \n",
    "        max_gen_length=15, top_p=.5, temperature=.9)\n",
    "    print(f\"Response {i}:\", tokenizer.decode(out_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab749686b4effeff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We observe that the language model has learned our _signals_ and _prompts_ we introduced.\n",
    "However, it is also apparent that the model tends to **hallucinate** â€” that is, it generates incorrect information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99dfc86a32c4a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "We observed that our model could hallucinate in some instances. Hallucinations in language models refer to generating incorrect or nonsensical information not supported by the input data. To mitigate this, the Retrieval Augmented Generation (RAG) approach involves providing an information system that adds relevant information to the input prompt based on our initial prompt. This helps ground the model's responses in factual data.\n",
    "\n",
    "The concept of Retrieval Augmented Generation was originally proposed in the paper  [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401). This approach combines traditional language modeling with a retrieval mechanism, which fetches relevant external information that can be used during the generation process. This combination allows the model to enhance its outputs with up-to-date and pertinent information, reducing the risk of generating inaccurate content.\n",
    "\n",
    "## Mechanism\n",
    "**Retrieval Component**: During text generation, RAG retrieves relevant documents or information from an external knowledge base.\n",
    "\n",
    "**Generation Component**: The retrieved information is then used to inform and improve the text generation process, grounding responses in factual data.\n",
    "\n",
    "<img src=\"../images/rag.jpg\" alt=\"Retrieval Augmented Generation Workflow\" />\n",
    "\n",
    "(illustration inspired by https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)\n",
    "\n",
    "## Benefits\n",
    "**Reduced Hallucinations**: By incorporating real-time, relevant information, RAG decreases the likelihood of generating incorrect or nonsensical outputs.\n",
    "\n",
    "**Enhanced Accuracy**: The retrieval process ensures that the model's outputs are more accurate and contextually appropriate.\n",
    "\n",
    "**Dynamic Knowledge Update**: Unlike static language models, RAG can dynamically incorporate new information, making it adaptable and up-to-date (for example financial news).\n",
    "\n",
    "## Application\n",
    "**Knowledge-Intensive Tasks**: RAG is particularly useful for tasks that require access to a large and dynamic body of knowledge, such as question answering, dialogue systems, and information retrieval.\n",
    "\n",
    "**Expert Systems**: It can be applied to domains requiring specialized knowledge, including company restricted knowledge, legal advice, and technical support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3fadb0b8f7695",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's start with manually adding some info to examine the impact on the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98534ae61eaaeb53",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    _, out_ids = generate(\n",
    "        peft_model, \n",
    "        tokenizer, \n",
    "        prompt_template({\n",
    "            \"input\": \"Answer the following question using the input: Berlin is the capital of Germany.\", \n",
    "            \"instruction\": \"What is the capital of germany?\"\n",
    "        }), \n",
    "        max_gen_length=20, \n",
    "        top_p=.5, \n",
    "        temperature=.9\n",
    "    )\n",
    "    print(f\"Response {i}:\", tokenizer.decode(out_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0229714ecdeb0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sentence Embedding and Semantic Search\n",
    "\n",
    "In our very limited test, we can see that adding relevant information has a positive effect on the quality of the response.\n",
    "\n",
    "--- \n",
    "\n",
    "We have seen already the concept of embeddings in a previous section: To recap, embeddings help a model determine how similar or different words are by placing them at certain distances from each other in a multidimensional vector space: similar words are placed closer together, and dissimilar words are farther apart from each other.\n",
    "\n",
    "Building on this concept, there are advanced models that apply a similar technique to entire sentences or even paragraphs, not just single words.\n",
    "\n",
    "**What Are Sentence Embeddings?**\n",
    "\n",
    "Sentence embeddings extend the idea of word embeddings to longer pieces of text. These models take whole sentences and encode them into numerical vectors in a vector space. By doing this, each sentence is represented by a point in a multidimensional vector space.\n",
    "\n",
    "**How Do Sentence Embeddings Encode Semantic Similarity?**\n",
    "\n",
    "The goal of sentence embeddings is to capture the overall meaning of a sentence, rather than just the meanings of individual words. This is achieved by considering the context of the entire sentence. For example, the sentences \"The weather is sunny\" and \"It's a bright sunny day\" share a similar theme and, thus, their vector representations would be placed close together in the embedding vector space.\n",
    "\n",
    "\n",
    "**Notes**\n",
    "\n",
    "Sentence embedding models, unlike the token embeddings we talked about earlier, are standalone models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccda8c7cd496aee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sentence transformers from Huggingface\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load a sentence embedding model from HuggingFace\n",
    "sent_embedder = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c363dec84805a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    \"What is the capital of germany.\", \n",
    "    \"The capital of germany is Graz.\"\n",
    "]\n",
    "\n",
    "sent_embedder.encode(example_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114417ee3fa8c36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Caused by the training method of sentence embedding models, _closeness_ is often not measured by the Euclidean distance but rather by the _cosine similarity_.\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "For two vectors $v_1, v_2 \\in \\mathbb{R}^d$, $v_1, v_2 \\neq 0$, we have the following identity\n",
    "\n",
    "$$\n",
    "\\cos(\\theta_{v_1, v_2}) = \\left\\langle \\frac{v_1}{\\|v_1\\|}, \\frac{v_2}{\\|v_2 \\|} \\right\\rangle = \\frac{1}{\\|v_1\\| \\|v_2\\|} \\sum_{i =1}^d v_{1,i}v_{2,i},\n",
    "$$\n",
    "where $v_1 = (v_{1,1},\\dots, v_{1, d}), \\, v_2 = (v_{2,1}, \\dots, v_{2,d})$, $\\|v_j \\| =  \\sqrt{\\langle v_j, v_j \\rangle}$ and $\\theta_{v_1, v_2}$ is one the two angles between $v_1$ and $v_2$.\n",
    "\n",
    "The _cosine similarity_ for $v_1$ and $v_2$, $v_1, v_2 \\neq 0$, is defined by \n",
    "\n",
    "$$\n",
    "\\mathrm{cosSim}(v_1, v_2) = \\left\\langle \\frac{v_1}{\\|v_1\\|}, \\frac{v_2}{\\|v_2 \\|} \\right\\rangle\n",
    "$$.\n",
    "\n",
    "Hence, $\\mathrm{cosSim}$ ranges between $[-1, 1]$.\n",
    "\n",
    "- If $v_1, v_2$ are unit vectors and $\\mathrm{cosSim}(v_1, v_2) = 1$, then $v_1 = v_2$.\n",
    "- If $v_1, v_2$ are unit vector and $\\mathrm{cosSim}(v_1, v_2) = -1$, then $v_1 = -v_2$.\n",
    "- If $v_1, v_2$ are unit vector and $\\mathrm{cosSim}(v_1, v_2) = 0$, then $v_1$ and $v_2$ are orthogonal.\n",
    "\n",
    "In summary, if two vector have a cosine simularity close to one, they are similar. The smaller the cosine similarity value, the less similar they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ef69601de9573",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v1 = np.array([0., 1., .8])\n",
    "v2 = np.array([0., 0., 1.])\n",
    "\n",
    "v1 = (1/np.linalg.norm(v1)) * v1\n",
    "v2 = (1/np.linalg.norm(v2)) * v2\n",
    "\n",
    "def add_3d_vector(x: np.ndarray, color: str, ax):\n",
    "    x = x.copy().reshape(-1)\n",
    "    \n",
    "    if x.shape == 3:\n",
    "        raise ValueError\n",
    "    \n",
    "    origin = np.zeros(shape=(3,))\n",
    "\n",
    "    x_vec = np.concatenate([origin.reshape(-1, 1), x.reshape(-1, 1)], axis=-1)\n",
    "    \n",
    "    ax.quiver(0, 0, 0, x_vec[0], x_vec[1], x_vec[2],color=color)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def add_unit_sphere(ax):\n",
    "    # make data\n",
    "    u = np.linspace(0, 2 * np.pi, 250)\n",
    "    v = np.linspace(0, np.pi, 250)\n",
    "    x = np.outer(np.cos(u), np.sin(v))\n",
    "    y = np.outer(np.sin(u), np.sin(v))\n",
    "    z = np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    \n",
    "    # plot the surface\n",
    "    ax.plot_surface(x, y, z, rstride=4, cstride=4, color='grey', linewidth=0, alpha=0.5)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def add_hyperplane(x1, x2, ax):\n",
    "    \n",
    "    u = np.linspace(-1, 1, 10)\n",
    "    v = np.linspace(-1, 1, 10)\n",
    "    um, vm = np.meshgrid(u, v)\n",
    "    \n",
    "    # generate points on the plane\n",
    "    xs = um * x1[0] + vm * x2[0]\n",
    "    ys = um * x1[1] + vm * x2[1]\n",
    "    zs = um * x1[2] + vm * x2[2]\n",
    "    \n",
    "    ax.plot_surface(xs, ys, zs, alpha=0.2, rstride=10, cstride=10, color=\"blue\")\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax = add_unit_sphere(ax)\n",
    "\n",
    "ax = add_3d_vector(v1, color=\"red\", ax=ax)\n",
    "ax = add_3d_vector(v2, color=\"green\", ax=ax)\n",
    "ax = add_hyperplane(v1, v2, ax=ax)\n",
    "\n",
    "ax.set_xlim((-1., 1.))\n",
    "ax.set_ylim((-1., 1.))\n",
    "ax.set_zlim((-1., 1.))\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8081cdf2bc665e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similar_sentences = [\n",
    "    \"The capital of germany is Berlin.\", \n",
    "    \"Berlin is the capital of Germany.\"\n",
    "]\n",
    "\n",
    "embeddings = sent_embedder.encode(similar_sentences)\n",
    "\n",
    "util.pytorch_cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c4205fac73143",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_similar_sentences = [\n",
    "    \"The capital of germany is Berlin.\", \n",
    "    \"I like large language models.\"\n",
    "]\n",
    "\n",
    "embeddings = sent_embedder.encode(non_similar_sentences)\n",
    "\n",
    "util.pytorch_cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c6f02509328dd4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Semantic Search and Vector Database\n",
    "\n",
    "Vector databases store data objects, such as text chunks, along with their vector representations or embeddings. This technique is not limited to text alone; it can also be applied to other data types like images, videos, and audio. By converting data into vector form, it becomes possible to perform queries based on semantic similarity rather than just exact matches or keywords.\n",
    "\n",
    "### How it works\n",
    "\n",
    "**Data Storage**:\n",
    "- Each data object (e.g., a text document, image, or video) is processed through a model to generate a vector representation. These vectors capture the semantic meaning of the data.\n",
    "- The vector representations are stored in the database alongside the original data.\n",
    "\n",
    "**Querying**:\n",
    "- When a query is made, it is first converted into a vector using the same model used for the data objects.\n",
    "- The database then compares this query vector with the stored vectors to find the most semantically similar data objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41756099287a58cf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load our poor man's document database\n",
    "def load_capital_db():\n",
    "    doc_format = \"The capital of {country} is {city}.\"\n",
    "    import json\n",
    "    \n",
    "    with open(os.path.join(\"..\", \"data\", \"country_by_capital_city.json\")) as file:\n",
    "        capital = json.load(file)\n",
    "        \n",
    "    capital = pd.DataFrame(capital)\n",
    "    \n",
    "    capital[\"text\"] = capital.apply(lambda row: doc_format.format(country=row[\"country\"], city=row[\"city\"] or \"unknown\"), axis=1)\n",
    "    \n",
    "    return capital[[\"text\"]]\n",
    "\n",
    "country_capital_documents = load_capital_db()\n",
    "country_capital_documents.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32940a93395df3b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_embedder.encode(country_capital_documents[\"text\"].tolist(), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ae4e28938c74b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_db = {\n",
    "    \"documents\": country_capital_documents,\n",
    "    \"embeddings\": sent_embedder.encode(country_capital_documents[\"text\"].tolist(), convert_to_tensor=True),\n",
    "    \"sentence_embedder\": sent_embedder,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1fd28d5f1d109",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_topk_documents(query: str, top_k: int = 10, score_threshold: float = .7):\n",
    "    \n",
    "    # embed query text\n",
    "    query_embedding = document_db[\"sentence_embedder\"].encode([query], convert_to_tensor=True)\n",
    "    \n",
    "    # calculate the cosine similarity of the query embedding to all document embeddings\n",
    "    cos_sim = util.dot_score(query_embedding, document_db[\"embeddings\"])[0]\n",
    "    \n",
    "    # get the most similar documents\n",
    "    scores, indices = torch.topk(cos_sim, k=top_k)\n",
    "    scores = [score.item() for score in scores if score.item() >= score_threshold]\n",
    "    indices = [indices.item() for i, indices in enumerate(indices) if i < len(scores)]\n",
    "    return scores, indices\n",
    "    \n",
    "scores, indices = retrieve_topk_documents(\"What is the capital of germany?\", top_k=3, score_threshold=0.4)\n",
    "print(scores)\n",
    "print(indices)\n",
    "document_db[\"documents\"].iloc[indices].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f91f677f6423d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question = \"The capital of germany is what?\"\n",
    "for i in range(10):\n",
    "    scores, indices = retrieve_topk_documents(question, top_k=1, score_threshold=0.8)\n",
    "    input_ = document_db[\"documents\"][\"text\"].iloc[indices].to_list()\n",
    "    prompt_input = {\"instruction\": question}\n",
    "    \n",
    "    if input_:\n",
    "        prompt_input[\"input\"] = input_[0]\n",
    "    \n",
    "    _, out_ids = generate(\n",
    "        peft_model, \n",
    "        tokenizer, \n",
    "        prompt_template(prompt_input), \n",
    "        max_gen_length=20, \n",
    "        top_p=.5, \n",
    "        temperature=.9, \n",
    "        alpha_penalty=0.01\n",
    "    )\n",
    "    print(f\"Response {i}:\", tokenizer.decode(out_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341d5e6ef8a2100",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7c541-94ac-470d-bdad-1009373b0bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aaf090-3bf6-4f05-92b6-5c293e0dcf39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
